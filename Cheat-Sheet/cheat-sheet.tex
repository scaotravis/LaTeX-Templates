% document properties
\documentclass[10pt,landscape]{article}
\title{Metrics Cheat Sheet}
\author{Travis Cao}

\usepackage{preamble}

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}

% multicol parameters
% These lengths are set only within the two main columns
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \large{710a Metrics Notes} \\
    \vspace{.1cm}
    \normalsize{Travis Cao}
\end{center}

\vspace{-.8cm}

\section{Stats}

\subsection{Geometric series sum}
$\sum_{j = 0}^n r^j = \frac{1 - r^{n+1}}{1 - r}$

\subsection{Conditional expectation}
$E[Y] = \sum_l E[Y|Z = l] Pr(Z = l)$

\subsection{Block inversion formula}
$M$ is invertible iff $A-BD^{-1}C$ is invertible, and $M^{-1} = $
\begin{align*}
  \begin{bmatrix}
    (A-BD^{-1}C)^{-1} & -(A-BD^{-1}C)^{-1}BD^{-1} \\ 
    -D^{-1}C(A-BD^{-1}C)^{-1} & D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
  \end{bmatrix}
\end{align*}

\subsection{Sherman-Morrison formula}
$A+uv'$ is invertible iff $1+v'A^{-1}u \neq 0$, and
\begin{align*}
  (A+uv')^{-1} = A^{-1} - \frac{A^{-1}uv'A^{-1}}{1+v'A^{-1}u}
\end{align*}

\subsection{Partial and average partial effect}
\begin{itemize}
  \item Partial effect (at $X = x$): $\frac{\partial}{\partial x} E[Y|X = x]$
  \item Average partial effect: $E[\frac{\partial}{\partial X} E[Y|X]]$
\end{itemize}

\subsection{Inequalities}
\begin{itemize}
  \item Chebyshev's: $Pr (\abs{X - E[X]} \geq \epsilon) \leq \frac{Var(X)}{\epsilon^2}$ for all $\epsilon > 0$
  \item Markov: $Pr(X \geq \epsilon) \leq \frac{E[X]}{\epsilon}$ for all $\epsilon > 0$
  \item Jensen's: For convex function $g(\cdot)$, $g(E[X]) \leq E[g(X)]$
  \item Cauchy Schwarz: $E[XY]^2 \leq E[X^2] E[Y^2]$
\end{itemize}


\subsection{Martingale CLT}
If $\{ Z_t \}_{t=1}^T$ satisfy
\begin{itemize}
  \vspace{3pt}
  \item $\{ Z_t \}_{t=1}^T$ is strictly stationary
  \vspace{3pt}
  \item $E[Z_1^2] < \infty$
  \vspace{3pt}
  \item $E[Z_t | Z_{t-1}, Z_{t-2}, \ldots, Z_1] = 0$
  \vspace{2pt}
  \item $\frac{1}{T} \sum_{t=1}^T Z_t^2 \xrightarrow{p} E[Z_1^2]$
\end{itemize}
then $\frac{1}{\sqrt{T}} \sum_{t=1}^T Z_t \xrightarrow{d} N(0, E[Z_1^2])$ as $T \rightarrow \infty$

% \vfill\null
% \columnbreak

\section{IV}

Model: $Y_i = X_i'\beta + U_i$

Assumptions: 
\begin{itemize}
  \item Validity of instrument
  \begin{itemize}
    \item Exogeneity: $E[U | Z] = 0$
    \item Relevance: $E[Z X']$ and $\sum_{i=1}^n Z_i X_i'$ are invertible
  \end{itemize}
  \item $E[|Y|^2 + ||X||^2 + ||Z||^2] < \infty$
  \item $\{ (Y_i, X_i', Z_i') \}$ are i.i.d.
\end{itemize}

IV estimator: $\hat{\beta}_{IV} = (\sum_{i=1}^n Z_i X_i')^{-1} (\sum_{i=1}^n Z_i Y_i')$

\subsection{Small sample property}
Model: $Y_i = \beta_0 + X_i \beta_1 + U_i$

\hspace{24pt} $X_i = \pi_0 + Z_i \pi_1 + V_i$

\vspace{3pt}
\hspace{24pt} $
  \begin{pmatrix}
    U_i \\ 
    V_i
  \end{pmatrix} \bigg| \enspace Z_i \sim N \left( 
    \begin{pmatrix}
      0 \\ 
      0
    \end{pmatrix}, 
    \begin{pmatrix}
      \sigma_u^2 & \sigma_{uv} \\ 
      \sigma_{uv} & \sigma_v^2
    \end{pmatrix} \right)
$
\vspace{3pt}

\then $E[\hat{\beta}_{IV} | \mathbbmss{X}, \mathbbmss{Z}] = \beta + \frac{\sigma_{uv}}{\sigma_v^2} \frac{\nu}{\frac{\hat{\pi}_1}{se(\hat{\pi}_1)} + \nu}$, where $\nu \sim N(0, 1)$

\subsection{Large sample property}

Large sample approximation is reasonable depends on both 
\begin{itemize}
  \item sample size $n$ (the bigger, the better), and
  \item the covariance between the instrument and endogenous regressor $\pi_1$ (the bigger, the better)
\end{itemize}

Which means there's a need of testing whether the instrument is relevant (whether $\pi_1 \neq 0$)

\begin{adjustwidth}{.3cm}{0cm}
  Model: $X_1 = Z_1 \pi_1 + X_2' \pi_2 + V$. Let $\hat{\pi}_1$ be OLS estimate of $\pi_1$. 

  Then the F-statistic is $F = \frac{(\hat{\pi}_1 - 0)^2}{se({\hat{\pi}_1})^2} = \frac{\hat{\pi}_1^2}{se({\hat{\pi}_1})^2}$

  Empirically, if $F \geq 10$, then some rough insurance with nominal $95\%$ conï¬dence intervals have actual coverage of at least $80\%$.
\end{adjustwidth}

\subsection{Weak instrument}

Model: $Y = X_1 \beta_1 + U$. Weak instrument is $Z_1$. 

Assumptions on $Z_1$: 
\begin{itemize}
  \item Exogeneity: $E[U|Z_1] = 0$
  \item Allow for relevance
  \item $E[Z_1^2]$ and $\frac{1}{n} \sum_i Z_{1i}^2$ are both non-zero (i.e. invertible)
\end{itemize}

Under weak instrument, want to test
\begin{align*}
  H_0 : \beta_1 = c \quad \quad H_1 : \beta_1 \neq c
\end{align*}

Identification under $H_0$: $E[Z_1(Y - X_1 c)] = 0$

\then $T = \frac{1}{n} \sum_i Z_{1i} (Y_i - X_{1i}c)$. Reject $H_0$ when $\abs{T}$ is large

Anderson-Rubin Test: 

\begin{adjustwidth}{.3cm}{0cm}
$AR \coloneqq \frac{\sqrt{n}T}{S} \xrightarrow{d} N(0, 1)$

where $S^2 = \frac{1}{n} Z_{1i}^2 (Y_i - X_{1i}c)^2 \xrightarrow{p} E[Z_1^2 U^2]$

With a size of $5\%$, reject $H_0$ when $\abs{AR} > 1.96$

Confidence set: $\{ c \in \RR : \abs{AR(c)} \leq 1.96 \}$
\end{adjustwidth}

\subsection{Optimal instrument}

Say optimal instrument is $h^*(Z)$. Identification yields from $E[h(Z) U] = 0$. So 
$
  \hat{\beta}^h_{IV} = (\frac{1}{n} \sum_{i} h(Z_i)X_i' )^{-1} \frac{1}{n}\sum_{i} h(Z_i) Y_i
$

Under homoskedasticity, asymptotic variance (AVAR) is 
\begin{align*}
  \Omega^h &= \frac{E[h(Z)^2]}{E[h(Z)X]^2} \sigma_u^2 = \frac{E[h(Z)^2]}{E[h(Z) E[X|Z]]^2} \sigma_u^2 \\
  &\geq \frac{E[h(Z)^2]}{E[h(Z)]^2 E[E[X|Z]^2]} \sigma_u^2 = \frac{\sigma_u^2}{E[E[X|Z]^2]}
\end{align*}
To achieve the lower bound of AVAR, 
$
  h^*(Z) = E[X|Z]
$

\subsection{Random coefficient model}

Model: $Y = X'\beta_0 \cdot U = X'\beta_0 + X'\beta_0 \cdot (U - 1)$

Assumptions: 
\begin{itemize}
  \item $E[U|Z] = 1$, and $E[ZX']$ invertible
  \item $Z$ independent of $Y$
  \item Let $X = (1, X)'$, $Z = (1, Z)'$, $Z \in \{ 0, 1 \}$, $X \in \{ 0, 1 \}$
\end{itemize}
\begin{align*}
  \hat{\beta}_{IV} \xrightarrow{p} \frac{Cov(Z, Y)}{Cov(Z, X)} = \frac{E[Y|Z = 1] - E[Y|Z = 0]}{E[X|Z = 1] - E[X|Z = 0]}
\end{align*}
For binary $X$, its response to $Z$ given unobservable $U$ is $X_U(Z)$
\begin{center}
  \begin{tabular}{c| c c}
    \toprule
    & $X_U(0) = 0$ & $X_U(0) = 1$ \\
    \midrule
    $X_U(1) = 0$ & Never taking & Defying \\
    $X_U(1) = 1$ & Complying & Always taking \\
    \bottomrule
  \end{tabular}
\end{center}
Assume no defyers, and $Pr(\text{Complying}) > 0$, then
\begin{align*}
  &E[X|Z = 1] - E[X|Z = 0] = E[X_U(1) - X_U(0)] \\ 
  &= Pr(X_U(1) - X_U(0) = 1) - Pr(X_U(1) - X_U(0) = -1) \\
  &= Pr(\text{Complying})\\ 
  &E[Y|Z = 1] - E[Y|Z = 0] = E[Y_U(1)X_U(1) + Y_U(0)(1-X_U(1))] \\
  &\quad - E[Y_U(1)X_U(0) + Y_U(0)(1-X_U(0))] \tag{since $Y = Y_U(1)X + Y_U(0)(1-X)$}\\
  &= E[(Y_U(1) - Y_U(0))(X_U(1) - X_U(0))] \\ 
  &= E[(Y_U(1) - Y_U(0))|X_U(1) - X_U(0) = 1] \times Pr(\text{Complying})
\end{align*}
Thus, $\hat{\beta}_{IV} \xrightarrow{p} E[\underbrace{(Y_U(1) - Y_U(0))}_{\text{mean response of $Y$ to $X$}}|\underbrace{X_U(1) - X_U(0) = 1}_{\text{for complyers}}]$

% \vfill\null
% \columnbreak

\section{Time Series}

\subsection{TS Models}
\begin{itemize}
  \item Static: $Y_t = \alpha_0 + X_t' \delta_0 + U_t$
  \item FDL($s$): $Y_t = \alpha_0 + X_t' \delta_0 + \ldots + X_{t-s}' \delta_s + U_t$ 
  \item AR($p$): $Y_t = \alpha_0 + Y_{t-1}\rho_1 + \ldots + Y_{t-p}\rho_p + U_t$
  \item MA($q$): $Y_t = \e_t + \theta_1 \e_{t-1} + \ldots + \theta_q \e_{t-q}$
  \item Trend: 
    \begin{itemize}
      \item Linear time trend: $Y_t = \beta_0 + \beta_1 t + U_t$
      \item Exponential trend: $\log(Y_t) = \beta_0 + \beta_1 t + U_t$
    \end{itemize}
  \item Seasonality: $Y_t = \alpha_0 + \alpha_1 1_{\{t/12 \text{ is an integer}\}} + U_t$
\end{itemize}

\subsection{Stationarity}
\begin{itemize}
  \item Strict: $(Y_{t_1}, Y_{t_2}, \ldots, Y_{t_k}) \sim (Y_{t_1+l}, Y_{t_2+l}, \ldots, Y_{t_k+l})$
  
  \vspace{-1.5pt}
  (Joint distribution is $t$ independent)

  \item Weak (Covariance): For all $t$, $E[Y_t]$ and $\gamma(k) = Cov(Y_t, Y_{t+k})$ ($\leftarrow$ autocovariance function) are both independent of $t$
\end{itemize}

\end{multicols}

\end{document}